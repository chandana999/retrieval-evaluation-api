# Advanced Retrieval Evaluation

A web-based UI for evaluating multiple RAG retrieval strategies using **RAGAS** (Retrieval Augmented Generation Assessment) metrics. Compare naive, BM25, contextual compression, multi-query, parent document, and ensemble retrievers side-by-side with real-time execution logs and an evaluation summary.

---

## ğŸ“¸ Execution Screenshot

![Evaluation UI](./advancedretrieval.png)


---

## ğŸ›  Tech Stack

| Layer | Technology |
|-------|------------|
| **Backend** | FastAPI, Uvicorn, Python 3.13 |
| **Frontend** | Vanilla HTML/CSS/JS (no framework) |
| **RAG / LLM** | LangChain, LangChain OpenAI, LangChain Cohere |
| **Vector Store** | Qdrant (in-memory) |
| **Evaluation** | RAGAS 0.3.6 |
| **Retrieval** | BM25 (rank-bm25), Qdrant, Cohere Rerank |
| **Data** | Pandas, CSV |

---

## ğŸ— High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (Browser)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Workflow     â”‚  â”‚ Evaluation       â”‚  â”‚ Retriever Comparison     â”‚   â”‚
â”‚  â”‚ Tracker      â”‚  â”‚ Summary          â”‚  â”‚ (Context Recall, etc.)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Status Log (SSE stream)   â”‚  â”‚ Raw Evaluation Data (JSON)           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ HTTP / SSE
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend (api/main.py)                         â”‚
â”‚  /api/run/stream  â”‚  /api/results  â”‚  /api/executive-summary             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ Subprocess (avoids asyncio conflicts)
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Evaluation Runner (api/evaluation_runner.py)                â”‚
â”‚  - Load dataset (Projects_with_Domains.csv)                              â”‚
â”‚  - Generate test dataset (RAGAS TestsetGenerator)                        â”‚
â”‚  - Build 6 retriever chains                                              â”‚
â”‚  - Evaluate each with RAGAS                                              â”‚
â”‚  - Save results to CSV                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              evaluate_retrievers.py + RAGAS + LangChain                  â”‚
â”‚  Naive â”‚ BM25 â”‚ Contextual Compression â”‚ Multi-Query â”‚ Parent Doc â”‚ Ensemble â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

- **6 Retrieval Strategies Evaluated:**
  - Naive (vector similarity)
  - BM25 (sparse / keyword)
  - Contextual compression (Cohere rerank)
  - Multi-query
  - Parent document
  - Ensemble (equal-weighted combination)

- **RAGAS Metrics:**
  - Context Recall
  - Context Entity Recall
  - Noise Sensitivity
  - Latency (per retriever)

- **UI:**
  - Workflow tracker (Loading â†’ Building â†’ Evaluating â†’ Summary)
  - Live execution log via Server-Sent Events (SSE)
  - Evaluation summary (refreshes on Run or Load Previous Results)
  - Retriever comparison table
  - Raw JSON for inspection

- **Results:** Auto-load from `data/` or `temporary/` on page load; manual "Load Previous Results" available.

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd 09_Advanced_Retrieval
uv sync
# or: pip install -e .
```

### 2. Configure API Keys

Edit `config.py` and set your keys:

- `OPENAI_API_KEY` (required)
- `COHERE_API_KEY` (required for contextual compression rerank)
- `LANGCHAIN_API_KEY` (optional, for LangSmith tracing)

### 3. Run the UI

```bash
python run_ui.py
```

Then open **http://localhost:8001** in your browser.

---

## ğŸ“ Project Structure

```
09_Advanced_Retrieval/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py              # FastAPI app, SSE streaming, static serving
â”‚   â””â”€â”€ evaluation_runner.py # Wraps evaluate_retrievers with log streaming
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # Main UI
â”‚   â””â”€â”€ assets/
â”‚       â”œâ”€â”€ style.css        # Styles
â”‚       â””â”€â”€ app.js           # Client logic (SSE, tables, summary)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Projects_with_Domains.csv           # Input dataset
â”‚   â””â”€â”€ retriever_evaluation_results.csv    # Output (after run)
â”œâ”€â”€ temporary/               # Optional alternate results path
â”œâ”€â”€ config.py                # API keys (edit here only)
â”œâ”€â”€ run_ui.py                # Entry point (port 8001)
â”œâ”€â”€ evaluate_retrievers.py   # Core retriever + RAGAS logic
â”œâ”€â”€ dataset_persistence.py   # Dataset caching
â”œâ”€â”€ testset_persistence.py   # Test dataset caching
â”œâ”€â”€ pyproject.toml           # Dependencies
â””â”€â”€ README.md                # This file
```

---

## ğŸ”‘ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Serves the UI |
| `/api/health` | GET | Health check |
| `/api/run/stream` | GET | Run evaluation, stream logs + retriever results via SSE |
| `/api/results` | GET | Get saved evaluation results (CSV â†’ JSON) |
| `/api/executive-summary` | GET | Get executive summary from saved results |
| `/api/status` | GET | Check if evaluation is running |

---

## ğŸ“Š Sample Output

| Retriever | Context Recall | Entity Recall | Noise Sensitivity | Latency (s) |
|-----------|----------------|---------------|-------------------|-------------|
| naive_retriever | 94.59% | 39.95% | 0% | 1305.85 |
| bm25_retriever | 79.59% | 36.86% | 0% | 695.13 |
| contextual_compression_retriever | 84.59% | 45.16% | 0% | 539.99 |
| multi_query_retriever | 94.59% | 43.70% | 0% | 1418.08 |
| parent_document_retriever | 84.59% | 36.07% | 0% | 1062.09 |
| ensemble_retriever | 94.59% | 45.57% | 0% | 2022.09 |

---

## ğŸ“ Notes

- **Evaluation runs in a subprocess** to avoid asyncio conflicts when stopping the server (Ragas uses async internally).
- Results are read from `data/` first, then `temporary/` if no `data/` file exists.
- LangSmith tracing is optional; cost/token stats appear when configured.

---


