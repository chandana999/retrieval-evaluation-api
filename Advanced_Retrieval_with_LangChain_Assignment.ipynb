{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\") or getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Use Case Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/Projects_with_Domains.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Project Title\",\n",
        "      \"Project Domain\",\n",
        "      \"Secondary Domain\",\n",
        "      \"Description\",\n",
        "      \"Judge Comments\",\n",
        "      \"Score\",\n",
        "      \"Project Name\",\n",
        "      \"Judge Score\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthetic_usecase_data = loader.load()\n",
        "\n",
        "for doc in synthetic_usecase_data:\n",
        "    doc.page_content = doc.metadata[\"Description\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/Projects_with_Domains.csv', 'row': 0, 'Project Title': 'InsightAI 1', 'Project Domain': 'Security', 'Secondary Domain': 'Finance / FinTech', 'Description': 'A low-latency inference system for multimodal agents in autonomous systems.', 'Judge Comments': 'Technically ambitious and well-executed.', 'Score': '85', 'Project Name': 'Project Aurora', 'Judge Score': '9.5'}, page_content='A low-latency inference system for multimodal agents in autonomous systems.')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "synthetic_usecase_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"Synthetic_Usecases\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    synthetic_usecase_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecases\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Healthcare / MedTech,\" which is mentioned multiple times.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security. Specifically, one project titled \"Pathfinder 24\" is described as an \"AI-powered platform optimizing logistics routes for sustainability\" with the secondary domain of security.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had various comments about the fintech-related projects. For example, the project \"TrendLens 19\" was described as \"Technically ambitious and well-executed,\" and \"Pathfinder 27\" received praise for \"Excellent code quality and use of open-source libraries,\" with a high judge score of 9.8. Another project, \"SecureNest 28,\" was noted as \"Conceptually strong but results need more benchmarking,\" with a judge score of 9.0. Overall, judges recognized these fintech projects for their technical ambition, quality, and impact, though some noted areas for improvement in benchmarking and results.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(synthetic_usecase_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Finance / FinTech,\" as it is explicitly mentioned in multiple entries.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a use case related to security. The project \"SecureNest\" in the domain of Eâ€‘commerce / Marketplaces and Secondary Domain of Legal / Compliance focuses on a document summarization and retrieval system for enterprise knowledge bases, which can be associated with security and compliance considerations.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges described the fintech projects positively. Specifically, for the project \"SynthMind,\" which is in the finance/fintech domain, the judges said it had a \"conceptually strong\" approach, although it needed more benchmarking of results.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### âœ… Answer\n",
        "BM25 is a traditional full-text search algorithm. It is useful when dealing with queries that rely on exact keyword matches. BM25 uses both term frequency - how often the term appears in the document and IDF - Inverse Document Frequency, how rare the term is across all documents, to identify documents.\n",
        "\n",
        "It finds use in web searches, e-commerce websites and document retrieval.\n",
        "\n",
        "An example suitable for BM25 is: \"Find projects that use federated learning toolkit\"\n",
        "\n",
        "BM25 finds matches for the following reasons:\n",
        "1. Exact terminilogy: BM25 finds exact matches for \"federated\", \"learning\", and \"toolkit\"\n",
        "2. Rare terms \"federated\" appears in 4 documents, giving it massive BM25 weight\n",
        "3. Technical precision: user wants this specific technique, not semantically similar ones\n",
        "4. Embedding risk: might return generic \"ML frameworks\" or \"privacy tools\" instead\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The projects that use a federated learning toolkit are:\\n\\n1. SkyForge\\n2. SynthMind\\n3. GreenPulse\\n4. PixelSense\\n\\nThese projects are noted for improving privacy in healthcare applications using federated learning techniques.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Find projects that use federated learning toolkit\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Synthetic_Usecases,\" as all the examples listed are related to synthetic data generators for various low-resource domain adaptation tasks. However, among the specific project domains mentioned in the sample, the ones listed are \"Productivity Assistants,\" \"Security,\" and \"Healthcare / MedTech.\" \\n\\nSince the question asks about the most common project domain overall, and considering the context shows multiple entries under \"Synthetic_Usecases\" (which seems to be the collection or category), I would say that \"Synthetic_Usecases\" is the most common project domain in this dataset.\\n\\nIf you\\'re referring specifically to the \"Project Domain\" field within the documents, the sample shows variability, and I do not have enough data to determine a single most common domain solely from the dataset. Based on the context, the overarching collection appears to focus on synthetic data use cases.\\n\\nTherefore, the most common project domain, given the context, is likely related to \"Synthetic_Usecases\" as the collection category, or within the Project Domains listed, there isn\\'t enough information to identify a dominant one.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific use cases related to security mentioned in the documents. The projects focus on areas like privacy in healthcare applications through federated learning, but there is no explicit mention of security-related use cases.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had a positive view of the fintech projects. Specifically, for the project \"Pathfinder 27\" in the Finance / FinTech domain, the judges commented on the \"excellent code quality and use of open-source libraries,\" indicating a high level of technical proficiency. The project also received a high judge score of 9.8 out of 10.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Healthcare / MedTech,\" which is mentioned multiple times among the projects.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, one project titled \"EchoLens\" is described as a hardware-aware model quantization benchmark suite, which is related to security in the context of legal and compliance domains.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had mixed feedback on the fintech projects. For the project \"Pathfinder 27,\" judges praised it for excellent code quality and use of open-source libraries, giving it a high score of 9.8. Overall, the comments suggest positive recognition for technical quality and innovation in the fintech projects.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### âœ… Answer\n",
        "Each query reformulation captures different semantic nuances. So, together they cover more semantic space\n",
        "\n",
        "1. Synonym coverage: Different words for the same concept\n",
        "2. Semantic diversity: Multiple embedding vectors cover more space\n",
        "3. Ambiguity resolution: Different interpretations of the query\n",
        "4. Terminology variations: Abbreviations, full terms, alternatives\n",
        "5. Perspective shifts: Different angles on the same question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = synthetic_usecase_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Healthcare / MedTech,\" as it is listed in at least one of the sample entries. However, since only a few entries are shown, I cannot definitively determine the most common domain overall. If you have access to the full dataset, you could analyze the frequency of each domain to find the most common one.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific use cases about security explicitly mentioned. The projects primarily focus on federated learning to improve privacy in healthcare applications, but explicit security use cases are not detailed.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges\\' comments about the fintech projects were generally positive. They described the projects as promising, clever, comprehensive, and technically mature. For example, one project was called \"promising idea with robust experimental validation,\" another was noted for being \"a clever solution with measurable environmental benefit,\" and others were described as \"technically ambitious and well-executed\" or \"comprehensive and technically mature.\"'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data is \"Finance / FinTech,\" which appears multiple times.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security. For example, one project is \"PixelSense,\" which is described as a \"document summarization and retrieval system for enterprise knowledge bases,\" involving legal and compliance domains, which can be related to security and data privacy.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges\\' comments on the fintech projects were generally positive, highlighting their strength and potential. For example, one fintech project, SynthMind, was described as having a \"conceptually strong\" approach, though it needed more benchmarking results. Another fintech project, PulseAI, was noted for being \"technically ambitious and well-executed.\" Overall, the judges recognized the innovative ideas and technical quality of the fintech projects.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(synthetic_usecase_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecase_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Legal / Compliance,\" as it is mentioned multiple times. However, given the small sample, I cannot definitively say it is the most frequent overall.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, one project named \"BioForge\" is a medical imaging solution in the security domain, and another project titled \"InsightAI 1\" called \"Project Aurora\" involves a low-latency inference system for multimodal agents in autonomous systems, which can be relevant to security applications.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges\\' comments about the fintech projects generally highlighted their technical ambition, execution, and potential impact. Specifically:\\n\\n- The project \"TrendLens 19\" was described as \"Technically ambitious and well-executed.\"\\n- \"WealthifyAI 16\" was noted for its \"Comprehensive and technically mature approach.\"\\n- For \"AutoMate 5,\" judges called it \"A forward-looking idea with solid supporting data.\"\\n\\nAdditionally, some projects received praise for increased clarity and potential, such as \"PlanPilot 7,\" which was described as having \"Great clarity in communication and demo flow.\" Overall, judges recognized the innovative and well-implemented nature of these fintech projects, emphasizing their technical quality and promising outlook.'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### âœ… Answer\n",
        "\n",
        "1. percentile\n",
        "\n",
        "How it works: Splits when distance exceeds the 95th percentile of all distances.\n",
        "\n",
        "- Best for FAQs\n",
        "- Only splits at major topic boundaries\n",
        "- Ignores small variations in phrasing\n",
        "- Groups similar Q&As together even if worded differently\n",
        "\n",
        "Use when: Questions are repetitive but you want to group similar topics\n",
        "\n",
        "\n",
        "2. standard_deviation\n",
        "\n",
        "How it works: Splits when distance > mean + (multiplier Ã— std_dev)\n",
        "- Too sensitive\n",
        "- Creates many small chunks\n",
        "- Splits on minor phrasing differences\n",
        "\n",
        "Use when: Content has consistent variation (not FAQs)\n",
        "\n",
        "3. interquartile\n",
        "\n",
        "How it works: Splits when distance > Q3 + (1.5 Ã— IQR)\n",
        "- Second best for FAQs\n",
        "- Robust to outliers\n",
        "- Groups similar questions\n",
        "- Less aggressive than percentile\n",
        "\n",
        "Use when: You want balanced chunking with outlier resistance\n",
        "\n",
        "4. gradient\n",
        "\n",
        "How it works: Splits when the change in distance suddenly increases\n",
        "- Can be inconsistent\n",
        "- Good at finding abrupt topic changes\n",
        "- May split unnecessarily if FAQs aren't ordered by topic\n",
        "- Works well if FAQs are grouped (all hours questions together)\n",
        "\n",
        "Use when: Content has clear narrative progression or topic clustering\n",
        "\n",
        "\n",
        "Best Choice: percentile (90-95th)\n",
        "\n",
        "- Groups similar questions regardless of phrasing\n",
        "- Only splits when topic truly changes\n",
        "- Handles repetition gracefully\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate_retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-10-13 21:11:16.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mLoading CSV dataset\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:16.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset_persistence\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mLoading from valid cache: 54e0f4b7c97d12d0727bba81fe55268b.pkl\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:16.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m404\u001b[0m - \u001b[1mGenerate test dataset\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:17.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtestset_persistence\u001b[0m:\u001b[36mgenerate_with_cache\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mLoading test dataset from cache (key: 8dcc5e6e...)\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:17.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtestset_persistence\u001b[0m:\u001b[36mgenerate_with_cache\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mCreated: 2025-10-11T21:22:02.058591\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:17.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtestset_persistence\u001b[0m:\u001b[36mgenerate_with_cache\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mSize: 10 samples\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:17.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtestset_persistence\u001b[0m:\u001b[36mgenerate_with_cache\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mDocs: 50\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:17.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m412\u001b[0m - \u001b[1mSize of test dataset - 10\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:17.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1mCreate Qdrant vector store\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:20.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m418\u001b[0m - \u001b[1mCreate rag prompt template\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:20.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m421\u001b[0m - \u001b[1mCreate chat model\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:40.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:40.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mEvaluating: naive_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:40.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:40.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mExpected project: Advanced-Retrieval-Eval-aae560d6-naive_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:40.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m470\u001b[0m - \u001b[1mGenerate evaluation dataset for `naive_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:11:40.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mcreate_evaluation_dataset\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mMake LCEL RAG chain for `naive_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:12:32.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mEvaluate `naive_retriever` using RAGAS\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efb24debc2a544e2b5645ffbeb2bd101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[2]: TimeoutError()\n",
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[20]: TimeoutError()\n",
            "Exception raised in Job[23]: TimeoutError()\n",
            "\u001b[32m2025-10-13 21:18:42.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mResults: {'context_recall': 1.0, 'context_entity_recall': 0.2941347537183415, 'noise_sensitivity_relevant': 0.28}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mLatency: 422.58s\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mEvaluating: bm25_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mExpected project: Advanced-Retrieval-Eval-aae560d6-bm25_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m470\u001b[0m - \u001b[1mGenerate evaluation dataset for `bm25_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:18:42.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mcreate_evaluation_dataset\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mMake LCEL RAG chain for `bm25_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:19:21.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mEvaluate `bm25_retriever` using RAGAS\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6dfe5f1fbc8430abacbe534b6e8723a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[2]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[23]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "\u001b[32m2025-10-13 21:25:27.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mResults: {'context_recall': 0.85, 'context_entity_recall': 0.36464313417168415, 'noise_sensitivity_relevant': 0.08521303258145363}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mLatency: 404.40s\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mEvaluating: contextual_compression_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mExpected project: Advanced-Retrieval-Eval-aae560d6-contextual_compression_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m470\u001b[0m - \u001b[1mGenerate evaluation dataset for `contextual_compression_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:25:27.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mcreate_evaluation_dataset\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mMake LCEL RAG chain for `contextual_compression_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:26:40.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mEvaluate `contextual_compression_retriever` using RAGAS\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c15372a062bc4008bb6f0904f409f8b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[20]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "\u001b[32m2025-10-13 21:32:18.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mResults: {'context_recall': 1.0, 'context_entity_recall': 0.4596481286020039, 'noise_sensitivity_relevant': 0.2094944150499706}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mLatency: 411.23s\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mEvaluating: multi_query_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mExpected project: Advanced-Retrieval-Eval-aae560d6-multi_query_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m470\u001b[0m - \u001b[1mGenerate evaluation dataset for `multi_query_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:32:18.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mcreate_evaluation_dataset\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mMake LCEL RAG chain for `multi_query_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:34:04.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mEvaluate `multi_query_retriever` using RAGAS\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d2b1812eb4448f59bc426110ac02a8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[29]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[2]: TimeoutError()\n",
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[20]: TimeoutError()\n",
            "Exception raised in Job[23]: TimeoutError()\n",
            "\u001b[32m2025-10-13 21:40:14.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mResults: {'context_recall': 1.0, 'context_entity_recall': 0.31809829015486324, 'noise_sensitivity_relevant': 0.2669172932330827}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mLatency: 476.45s\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mEvaluating: parent_document_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mExpected project: Advanced-Retrieval-Eval-aae560d6-parent_document_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m470\u001b[0m - \u001b[1mGenerate evaluation dataset for `parent_document_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:40:14.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mcreate_evaluation_dataset\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mMake LCEL RAG chain for `parent_document_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:41:39.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mEvaluate `parent_document_retriever` using RAGAS\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7663c831365a4dccac046ddc2991a1a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-10-13 21:47:12.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mResults: {'context_recall': 1.0, 'context_entity_recall': 0.30216394678622677, 'noise_sensitivity_relevant': 0.10478773273083215}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mLatency: 418.20s\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mEvaluating: ensemble_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mExpected project: Advanced-Retrieval-Eval-aae560d6-ensemble_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m470\u001b[0m - \u001b[1mGenerate evaluation dataset for `ensemble_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:47:12.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mcreate_evaluation_dataset\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mMake LCEL RAG chain for `ensemble_retriever`\u001b[0m\n",
            "\u001b[32m2025-10-13 21:49:58.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mEvaluate `ensemble_retriever` using RAGAS\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc3b78adc5b94fa88fe8efaa2938605a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[2]: TimeoutError()\n",
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[8]: TimeoutError()\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[20]: TimeoutError()\n",
            "Exception raised in Job[23]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "\u001b[32m2025-10-13 21:56:12.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mResults: {'context_recall': 1.0, 'context_entity_recall': 0.3172649568272938, 'noise_sensitivity_relevant': 0.37896825396825395}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:56:12.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mLatency: 539.77s\u001b[0m\n",
            "\u001b[32m2025-10-13 21:56:12.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m514\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:56:12.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m515\u001b[0m - \u001b[1mFETCHING COST DATA FROM LANGSMITH\u001b[0m\n",
            "\u001b[32m2025-10-13 21:56:12.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m516\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 21:56:12.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mSession ID: aae560d6\u001b[0m\n",
            "\u001b[32m2025-10-13 21:56:20.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mFound 373 tagged runs for naive_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:57:11.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mFound 11 ragas evaluation runs\u001b[0m\n",
            "\u001b[32m2025-10-13 21:57:11.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1m{'total_cost': Decimal('0.3543604'), 'total_tokens': 458689, 'prompt_tokens': 304030, 'completion_tokens': 154659, 'run_count': 11, 'total_latency_seconds': 419.8193, 'avg_latency_seconds': 38.16539090909091}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:57:11.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mnaive_retriever:\u001b[0m\n",
            "\u001b[32m2025-10-13 21:57:11.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[1m  Total Cost: $0.3544\u001b[0m\n",
            "\u001b[32m2025-10-13 21:57:11.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1m  Tokens: 458,689\u001b[0m\n",
            "\u001b[32m2025-10-13 21:57:24.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mFound 281 tagged runs for bm25_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:58:08.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mFound 11 ragas evaluation runs\u001b[0m\n",
            "\u001b[32m2025-10-13 21:58:08.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1m{'total_cost': Decimal('0.2042969'), 'total_tokens': 256670, 'prompt_tokens': 165904, 'completion_tokens': 90766, 'run_count': 11, 'total_latency_seconds': 402.144392, 'avg_latency_seconds': 36.55858109090909}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:58:08.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mbm25_retriever:\u001b[0m\n",
            "\u001b[32m2025-10-13 21:58:08.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[1m  Total Cost: $0.2043\u001b[0m\n",
            "\u001b[32m2025-10-13 21:58:08.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1m  Tokens: 256,670\u001b[0m\n",
            "\u001b[32m2025-10-13 21:58:15.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mFound 273 tagged runs for contextual_compression_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 21:59:08.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mFound 11 ragas evaluation runs\u001b[0m\n",
            "\u001b[32m2025-10-13 21:59:08.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1m{'total_cost': Decimal('0.1965062'), 'total_tokens': 241127, 'prompt_tokens': 150867, 'completion_tokens': 90260, 'run_count': 11, 'total_latency_seconds': 408.14745600000003, 'avg_latency_seconds': 37.10431418181818}\u001b[0m\n",
            "\u001b[32m2025-10-13 21:59:08.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mcontextual_compression_retriever:\u001b[0m\n",
            "\u001b[32m2025-10-13 21:59:08.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[1m  Total Cost: $0.1965\u001b[0m\n",
            "\u001b[32m2025-10-13 21:59:08.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1m  Tokens: 241,127\u001b[0m\n",
            "\u001b[32m2025-10-13 21:59:36.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mFound 391 tagged runs for multi_query_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 22:00:17.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mFound 11 ragas evaluation runs\u001b[0m\n",
            "\u001b[32m2025-10-13 22:00:17.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1m{'total_cost': Decimal('0.3895841'), 'total_tokens': 506027, 'prompt_tokens': 334501, 'completion_tokens': 171526, 'run_count': 11, 'total_latency_seconds': 473.720443, 'avg_latency_seconds': 43.06549481818182}\u001b[0m\n",
            "\u001b[32m2025-10-13 22:00:17.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mmulti_query_retriever:\u001b[0m\n",
            "\u001b[32m2025-10-13 22:00:17.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[1m  Total Cost: $0.3896\u001b[0m\n",
            "\u001b[32m2025-10-13 22:00:17.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1m  Tokens: 506,027\u001b[0m\n",
            "\u001b[32m2025-10-13 22:00:31.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mFound 291 tagged runs for parent_document_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 22:01:28.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mFound 11 ragas evaluation runs\u001b[0m\n",
            "\u001b[32m2025-10-13 22:01:28.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1m{'total_cost': Decimal('0.2358499'), 'total_tokens': 285475, 'prompt_tokens': 175417, 'completion_tokens': 110058, 'run_count': 11, 'total_latency_seconds': 413.325875, 'avg_latency_seconds': 37.57507954545454}\u001b[0m\n",
            "\u001b[32m2025-10-13 22:01:28.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mparent_document_retriever:\u001b[0m\n",
            "\u001b[32m2025-10-13 22:01:28.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[1m  Total Cost: $0.2358\u001b[0m\n",
            "\u001b[32m2025-10-13 22:01:28.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1m  Tokens: 285,475\u001b[0m\n",
            "\u001b[32m2025-10-13 22:01:53.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mFound 419 tagged runs for ensemble_retriever\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mget_langsmith_cost_stats\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mFound 11 ragas evaluation runs\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1m{'total_cost': Decimal('0.4208468'), 'total_tokens': 557117, 'prompt_tokens': 372626, 'completion_tokens': 184491, 'run_count': 11, 'total_latency_seconds': 536.5057380000001, 'avg_latency_seconds': 48.77324890909092}\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mensemble_retriever:\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[1m  Total Cost: $0.4208\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1m  Tokens: 557,117\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m562\u001b[0m - \u001b[1m\n",
            "============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m563\u001b[0m - \u001b[1mFINAL COMPARISON\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1m============================================================\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1m\n",
            "Results saved to: ./data/retriever_evaluation_results.csv\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m572\u001b[0m - \u001b[1m\n",
            "For detailed cost analysis, check LangSmith dashboard at:\u001b[0m\n",
            "\u001b[32m2025-10-13 22:03:00.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate_retrievers\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m573\u001b[0m - \u001b[1mhttps://smith.langchain.com/\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "evaluate_retrievers.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# Check evaluate_retrievers.py\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/retriever_evaluation_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['avg_performance'] = (df[\"context_recall\"] + df[\"context_entity_recall\"] + df[\"noise_sensitivity\"])/3\n",
        "df['norm_avg_performance'] = (df['avg_performance'] - df['avg_performance'].min()) / (df['avg_performance'].max() - df['avg_performance'].min())\n",
        "df['norm_latency_seconds'] = (df['langsmith_total_latency'] - df['langsmith_total_latency'].min()) / (df['langsmith_total_latency'].max() - df['langsmith_total_latency'].min())\n",
        "df['norm_cost_usd'] = (df['langsmith_total_cost_usd'] - df['langsmith_total_cost_usd'].min()) / (df['langsmith_total_cost_usd'].max() - df['langsmith_total_cost_usd'].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_4402f_row0_col1, #T_4402f_row1_col8, #T_4402f_row2_col1, #T_4402f_row2_col2, #T_4402f_row2_col5, #T_4402f_row3_col1, #T_4402f_row4_col1, #T_4402f_row5_col1, #T_4402f_row5_col3, #T_4402f_row5_col9 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "#T_4402f_row0_col2, #T_4402f_row1_col1, #T_4402f_row1_col3, #T_4402f_row1_col9, #T_4402f_row5_col5, #T_4402f_row5_col8 {\n",
              "  background-color: lightcoral;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_4402f\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4402f_level0_col0\" class=\"col_heading level0 col0\" >retriever</th>\n",
              "      <th id=\"T_4402f_level0_col1\" class=\"col_heading level0 col1\" >context_recall</th>\n",
              "      <th id=\"T_4402f_level0_col2\" class=\"col_heading level0 col2\" >context_entity_recall</th>\n",
              "      <th id=\"T_4402f_level0_col3\" class=\"col_heading level0 col3\" >noise_sensitivity</th>\n",
              "      <th id=\"T_4402f_level0_col4\" class=\"col_heading level0 col4\" >latency_seconds</th>\n",
              "      <th id=\"T_4402f_level0_col5\" class=\"col_heading level0 col5\" >langsmith_total_cost_usd</th>\n",
              "      <th id=\"T_4402f_level0_col6\" class=\"col_heading level0 col6\" >langsmith_total_tokens</th>\n",
              "      <th id=\"T_4402f_level0_col7\" class=\"col_heading level0 col7\" >langsmith_llm_calls</th>\n",
              "      <th id=\"T_4402f_level0_col8\" class=\"col_heading level0 col8\" >langsmith_total_latency</th>\n",
              "      <th id=\"T_4402f_level0_col9\" class=\"col_heading level0 col9\" >avg_performance</th>\n",
              "      <th id=\"T_4402f_level0_col10\" class=\"col_heading level0 col10\" >norm_avg_performance</th>\n",
              "      <th id=\"T_4402f_level0_col11\" class=\"col_heading level0 col11\" >norm_latency_seconds</th>\n",
              "      <th id=\"T_4402f_level0_col12\" class=\"col_heading level0 col12\" >norm_cost_usd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4402f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_4402f_row0_col0\" class=\"data row0 col0\" >naive_retriever</td>\n",
              "      <td id=\"T_4402f_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row0_col2\" class=\"data row0 col2\" >0.294135</td>\n",
              "      <td id=\"T_4402f_row0_col3\" class=\"data row0 col3\" >0.280000</td>\n",
              "      <td id=\"T_4402f_row0_col4\" class=\"data row0 col4\" >422.575179</td>\n",
              "      <td id=\"T_4402f_row0_col5\" class=\"data row0 col5\" >0.354360</td>\n",
              "      <td id=\"T_4402f_row0_col6\" class=\"data row0 col6\" >458689</td>\n",
              "      <td id=\"T_4402f_row0_col7\" class=\"data row0 col7\" >11</td>\n",
              "      <td id=\"T_4402f_row0_col8\" class=\"data row0 col8\" >419.819300</td>\n",
              "      <td id=\"T_4402f_row0_col9\" class=\"data row0 col9\" >0.524712</td>\n",
              "      <td id=\"T_4402f_row0_col10\" class=\"data row0 col10\" >0.691964</td>\n",
              "      <td id=\"T_4402f_row0_col11\" class=\"data row0 col11\" >0.131548</td>\n",
              "      <td id=\"T_4402f_row0_col12\" class=\"data row0 col12\" >0.703636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4402f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_4402f_row1_col0\" class=\"data row1 col0\" >bm25_retriever</td>\n",
              "      <td id=\"T_4402f_row1_col1\" class=\"data row1 col1\" >0.850000</td>\n",
              "      <td id=\"T_4402f_row1_col2\" class=\"data row1 col2\" >0.364643</td>\n",
              "      <td id=\"T_4402f_row1_col3\" class=\"data row1 col3\" >0.085213</td>\n",
              "      <td id=\"T_4402f_row1_col4\" class=\"data row1 col4\" >404.397298</td>\n",
              "      <td id=\"T_4402f_row1_col5\" class=\"data row1 col5\" >0.204297</td>\n",
              "      <td id=\"T_4402f_row1_col6\" class=\"data row1 col6\" >256670</td>\n",
              "      <td id=\"T_4402f_row1_col7\" class=\"data row1 col7\" >11</td>\n",
              "      <td id=\"T_4402f_row1_col8\" class=\"data row1 col8\" >402.144392</td>\n",
              "      <td id=\"T_4402f_row1_col9\" class=\"data row1 col9\" >0.433285</td>\n",
              "      <td id=\"T_4402f_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n",
              "      <td id=\"T_4402f_row1_col11\" class=\"data row1 col11\" >0.000000</td>\n",
              "      <td id=\"T_4402f_row1_col12\" class=\"data row1 col12\" >0.034727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4402f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_4402f_row2_col0\" class=\"data row2 col0\" >contextual_compression_retriever</td>\n",
              "      <td id=\"T_4402f_row2_col1\" class=\"data row2 col1\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row2_col2\" class=\"data row2 col2\" >0.459648</td>\n",
              "      <td id=\"T_4402f_row2_col3\" class=\"data row2 col3\" >0.209494</td>\n",
              "      <td id=\"T_4402f_row2_col4\" class=\"data row2 col4\" >411.234186</td>\n",
              "      <td id=\"T_4402f_row2_col5\" class=\"data row2 col5\" >0.196506</td>\n",
              "      <td id=\"T_4402f_row2_col6\" class=\"data row2 col6\" >241127</td>\n",
              "      <td id=\"T_4402f_row2_col7\" class=\"data row2 col7\" >11</td>\n",
              "      <td id=\"T_4402f_row2_col8\" class=\"data row2 col8\" >408.147456</td>\n",
              "      <td id=\"T_4402f_row2_col9\" class=\"data row2 col9\" >0.556381</td>\n",
              "      <td id=\"T_4402f_row2_col10\" class=\"data row2 col10\" >0.931654</td>\n",
              "      <td id=\"T_4402f_row2_col11\" class=\"data row2 col11\" >0.044679</td>\n",
              "      <td id=\"T_4402f_row2_col12\" class=\"data row2 col12\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4402f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_4402f_row3_col0\" class=\"data row3 col0\" >multi_query_retriever</td>\n",
              "      <td id=\"T_4402f_row3_col1\" class=\"data row3 col1\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row3_col2\" class=\"data row3 col2\" >0.318098</td>\n",
              "      <td id=\"T_4402f_row3_col3\" class=\"data row3 col3\" >0.266917</td>\n",
              "      <td id=\"T_4402f_row3_col4\" class=\"data row3 col4\" >476.448286</td>\n",
              "      <td id=\"T_4402f_row3_col5\" class=\"data row3 col5\" >0.389584</td>\n",
              "      <td id=\"T_4402f_row3_col6\" class=\"data row3 col6\" >506027</td>\n",
              "      <td id=\"T_4402f_row3_col7\" class=\"data row3 col7\" >11</td>\n",
              "      <td id=\"T_4402f_row3_col8\" class=\"data row3 col8\" >473.720443</td>\n",
              "      <td id=\"T_4402f_row3_col9\" class=\"data row3 col9\" >0.528339</td>\n",
              "      <td id=\"T_4402f_row3_col10\" class=\"data row3 col10\" >0.719415</td>\n",
              "      <td id=\"T_4402f_row3_col11\" class=\"data row3 col11\" >0.532713</td>\n",
              "      <td id=\"T_4402f_row3_col12\" class=\"data row3 col12\" >0.860646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4402f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_4402f_row4_col0\" class=\"data row4 col0\" >parent_document_retriever</td>\n",
              "      <td id=\"T_4402f_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row4_col2\" class=\"data row4 col2\" >0.302164</td>\n",
              "      <td id=\"T_4402f_row4_col3\" class=\"data row4 col3\" >0.104788</td>\n",
              "      <td id=\"T_4402f_row4_col4\" class=\"data row4 col4\" >418.203961</td>\n",
              "      <td id=\"T_4402f_row4_col5\" class=\"data row4 col5\" >0.235850</td>\n",
              "      <td id=\"T_4402f_row4_col6\" class=\"data row4 col6\" >285475</td>\n",
              "      <td id=\"T_4402f_row4_col7\" class=\"data row4 col7\" >11</td>\n",
              "      <td id=\"T_4402f_row4_col8\" class=\"data row4 col8\" >413.325875</td>\n",
              "      <td id=\"T_4402f_row4_col9\" class=\"data row4 col9\" >0.468984</td>\n",
              "      <td id=\"T_4402f_row4_col10\" class=\"data row4 col10\" >0.270186</td>\n",
              "      <td id=\"T_4402f_row4_col11\" class=\"data row4 col11\" >0.083219</td>\n",
              "      <td id=\"T_4402f_row4_col12\" class=\"data row4 col12\" >0.175375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4402f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_4402f_row5_col0\" class=\"data row5 col0\" >ensemble_retriever</td>\n",
              "      <td id=\"T_4402f_row5_col1\" class=\"data row5 col1\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row5_col2\" class=\"data row5 col2\" >0.317265</td>\n",
              "      <td id=\"T_4402f_row5_col3\" class=\"data row5 col3\" >0.378968</td>\n",
              "      <td id=\"T_4402f_row5_col4\" class=\"data row5 col4\" >539.772266</td>\n",
              "      <td id=\"T_4402f_row5_col5\" class=\"data row5 col5\" >0.420847</td>\n",
              "      <td id=\"T_4402f_row5_col6\" class=\"data row5 col6\" >557117</td>\n",
              "      <td id=\"T_4402f_row5_col7\" class=\"data row5 col7\" >11</td>\n",
              "      <td id=\"T_4402f_row5_col8\" class=\"data row5 col8\" >536.505738</td>\n",
              "      <td id=\"T_4402f_row5_col9\" class=\"data row5 col9\" >0.565411</td>\n",
              "      <td id=\"T_4402f_row5_col10\" class=\"data row5 col10\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row5_col11\" class=\"data row5 col11\" >1.000000</td>\n",
              "      <td id=\"T_4402f_row5_col12\" class=\"data row5 col12\" >1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x12df51fd0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Highlight max/min only\n",
        "simple_highlight = df.style.highlight_max(\n",
        "    subset=['context_recall', 'context_entity_recall', 'noise_sensitivity', 'avg_performance'],\n",
        "    color='lightgreen'\n",
        ").highlight_min(\n",
        "    subset=['langsmith_total_latency', 'langsmith_total_cost_usd'],\n",
        "    color='lightgreen'\n",
        ").highlight_min(\n",
        "    subset=['context_recall', 'context_entity_recall', 'noise_sensitivity', 'avg_performance'],\n",
        "    color='lightcoral'\n",
        ").highlight_max(\n",
        "    subset=['langsmith_total_latency', 'langsmith_total_cost_usd'],\n",
        "    color='lightcoral'\n",
        ")\n",
        "\n",
        "simple_highlight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "WINNER: Contextual Compression offers the best cost-performance-latency balance, performing within 2% of the best (Ensemble) while being significantly cheaper and faster - within 2% of the fastest (BM25) retriever.\n",
        "\n",
        "Why:<br>\n",
        "**Performance Leader:** Ensemble (0.565)\n",
        "\n",
        "- Highest average performance\n",
        "- But: Expensive and slow\n",
        "\n",
        "**Speed Leader:** BM25 (402s)\n",
        "\n",
        "- Fastest retrieval\n",
        "- But: Lowest performance\n",
        "\n",
        "**Cost Leader:** Contextual Compression ($0.19)\n",
        "\n",
        "- Cheapest option\n",
        "- Decent performance (0.556) - only 1.6% worse than Ensemble (best performer)\n",
        "- Good latency (408s) - only 1.5% slower than BM25 (fastest)\n",
        "\n",
        "**Actual Winner:** It Depends on Priority\n",
        "\n",
        "If Performance is Critical â†’ Ensemble\n",
        "\n",
        "- Accept higher cost/latency for best results\n",
        "\n",
        "If Speed is Critical â†’ BM25\n",
        "\n",
        "- Accept poor performance for fastest retrieval\n",
        "\n",
        "If Balanced/Cost-Conscious â†’ Contextual Compression\n",
        "\n",
        "- Best cost-performance-latency tradeoff\n",
        "- Only marginally worse than Ensemble (0.556 vs 0.565)\n",
        "- Much cheaper and faster, only slight slower than BM25 (402s vs 408s)\n",
        "\n",
        "We could consider improving the results by adding lengthier descriptions as the original descriptions were less than 100 tokens. We could also consider increasing the testset sample size which could help improve performance albeit increasing both latency and cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
